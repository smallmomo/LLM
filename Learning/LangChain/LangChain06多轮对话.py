from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chat_models import init_chat_model
from langchain_core.output_parsers import StrOutputParser

from dotenv import load_dotenv  # å¯¼å…¥è¯»å–envçš„åº“
import os
# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()  # é»˜è®¤è¯»å–é¡¹ç›®æ ¹ç›®å½•çš„.envæ–‡ä»¶
model = init_chat_model(
    model=os.getenv("QWEN_MODEL"),  # ä»envè¯»å–æ¨¡å‹å
    model_provider="openai",
    base_url=os.getenv("SILICONFLOW_BASE_URL"),  # ä»envè¯»å–base_url
    api_key=os.getenv("SILICONFLOW_API_KEY"),  # ä»envè¯»å–api_key
)

parser = StrOutputParser()

prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="ä½ å«æ¨ç´«ï¼Œæ˜¯è‘—åå¥³æ¼”å‘˜ã€‚"),
    MessagesPlaceholder(variable_name="messages"),
])

chain = prompt | model | parser

messages_list = []  # åˆå§‹åŒ–å†å²
print("ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯")
while True:
    user_query = input("ä½ ï¼š")
    if user_query.lower() in {"exit", "quit"}:
        break

    # 1) è¿½åŠ ç”¨æˆ·æ¶ˆæ¯
    messages_list.append(HumanMessage(content=user_query))

    # 2) è°ƒç”¨æ¨¡å‹
    # assistant_reply = chain.invoke({"messages": messages_list})
    # print("æ¨ç´«ï¼š", assistant_reply)

    # 2) è°ƒç”¨æ¨¡å‹ æµå¼
    assistant_reply = ''
    print('æ¨ç´«:', end=' ')
    for chunk in chain.stream({"messages": messages_list}):
        assistant_reply += chunk
        print(chunk, end="", flush=True)
    print()


    # 3) è¿½åŠ  AI å›å¤
    messages_list.append(AIMessage(content=assistant_reply))

    # 4) ä»…ä¿ç•™æœ€è¿‘ 50 æ¡
    messages_list = messages_list[-50:]
